# üêù Performance-First Context Engine - Recommended Architecture

## **üéØ MY RECOMMENDATIONS:**

### **ARCHITECTURE: Two-Tier Context System**

#### **Tier 1: Hot Context (Real-Time Performance)**
- **Last 15 messages per thread** (not 10 - gives better context window)
- **Stored in Supabase** with aggressive indexing (no Redis needed initially)
- **Sub-50ms retrieval** for recent context
- **Zero AI processing** during chat interactions
- **Simple SQL queries** for maximum speed

#### **Tier 2: Cold Context (N8N Background Processing)**
- **Hourly batch processing** of conversations older than 15 messages
- **Keyword extraction + entity recognition** 
- **Semantic embeddings** for deep search
- **Conversation summaries** and relationship mapping
- **Full-text search index** for historical queries

## **üöÄ SPECIFIC RECOMMENDATIONS:**

### **1. Recent Message Storage (Tier 1)**
**RECOMMEND:** Last 15 messages **per thread** (not per user)
- **Why:** Thread-specific context is more relevant than cross-thread
- **Performance:** Single query: `SELECT * FROM messages WHERE thread_id = ? ORDER BY created_at DESC LIMIT 15`
- **Memory:** ~15KB per active thread (manageable)

### **2. N8N Processing Frequency**
**RECOMMEND:** Every 2 hours (not hourly)
- **Why:** Reduces N8N load, still provides timely indexing
- **Performance:** Allows larger batch sizes for efficiency
- **Cost:** Fewer OpenAI API calls for embeddings

### **3. Context Injection Strategy**
**RECOMMEND:** Automatic recent + on-demand historical
- **Recent (Tier 1):** Always inject last 5-10 relevant messages
- **Historical (Tier 2):** Only when user query suggests need for history
- **Smart triggers:** "Remember when...", "Last time we discussed...", entity mentions

### **4. Performance Targets**
**RECOMMEND:** 
- **Recent context retrieval:** <50ms
- **Chat response with context:** <200ms total
- **Historical context search:** <500ms (acceptable for complex queries)

### **5. Keyword vs Semantic**
**RECOMMEND:** Hybrid approach
- **Phase 1:** Keyword extraction + simple entity recognition (fast, reliable)
- **Phase 2:** Add semantic embeddings for complex queries
- **Fallback:** Always have keyword search as backup

### **6. Storage Strategy**
**RECOMMEND:** All Supabase with smart indexing
- **Why:** Simpler architecture, one database to manage
- **Hot data:** Composite indexes on (thread_id, created_at)
- **Cold data:** Full-text search indexes + vector indexes
- **Caching:** Application-level caching for frequently accessed threads

## **üèóÔ∏è IMPLEMENTATION PLAN:**

### **Phase 1: Hot Context (Week 1)**
```sql
-- Simple recent messages function
CREATE OR REPLACE FUNCTION get_recent_context(p_thread_id uuid, p_limit int DEFAULT 15)
RETURNS TABLE (role text, content text, created_at timestamptz)
AS $$
  SELECT role, content, created_at 
  FROM messages 
  WHERE thread_id = p_thread_id 
  ORDER BY created_at DESC 
  LIMIT p_limit;
$$;
```

### **Phase 2: Context Injection (Week 1)**
```typescript
// Fast context injection - no AI processing
export async function getRecentContext(threadId: string): Promise<ContextMessage[]> {
  const { data } = await supabaseAdmin
    .rpc('get_recent_context', { p_thread_id: threadId });
  
  return data || [];
}

// Inject into prompt (simple string concatenation)
export function injectRecentContext(prompt: string, context: ContextMessage[]): string {
  if (context.length === 0) return prompt;
  
  const contextStr = context
    .reverse() // chronological order
    .map(msg => `${msg.role}: ${msg.content}`)
    .join('\n');
    
  return `Previous conversation:\n${contextStr}\n\nCurrent query: ${prompt}`;
}
```

### **Phase 3: N8N Background Processing (Week 2)**
```json
{
  "name": "Hive Context Indexer",
  "trigger": {
    "type": "cron",
    "expression": "0 */2 * * *"
  },
  "workflow": [
    "Get unprocessed conversations (older than 15 messages)",
    "Extract keywords using simple regex patterns",
    "Extract entities using GPT-3.5 (batch processing)",
    "Store in context_index table",
    "Mark conversations as processed"
  ]
}
```

### **Phase 4: Historical Search (Week 3)**
```typescript
// Only when needed - triggered by specific query patterns
export async function searchHistoricalContext(
  orgId: string, 
  query: string
): Promise<ContextItem[]> {
  // Simple keyword search first
  const { data } = await supabaseAdmin
    .from('context_index')
    .select('*')
    .eq('org_id', orgId)
    .textSearch('keywords', query)
    .limit(10);
    
  return data || [];
}
```

## **üìä PERFORMANCE COMPARISON:**

### **My Original Design (Real-Time):**
- **Chat latency:** 200-500ms (embedding generation)
- **N8N load:** High (every message)
- **Database load:** High (constant writes)
- **Cost:** High (many OpenAI calls)

### **Recommended Design (Performance-First):**
- **Chat latency:** 50-100ms (simple SQL)
- **N8N load:** Low (every 2 hours)
- **Database load:** Low (batch processing)
- **Cost:** Low (fewer API calls)

## **üîí SECURITY BENEFITS:**

### **Reduced Attack Surface:**
- **No real-time webhooks** during chat
- **Batch processing** easier to monitor and secure
- **Simple SQL queries** less prone to injection
- **Fewer external API calls** during user interactions

### **Better Audit Trail:**
- **Clear separation** between real-time and background processing
- **Batch processing logs** easier to audit
- **Performance metrics** easier to track

## **üéØ MIGRATION STRATEGY:**

### **From Current Implementation:**
1. **Keep the database schema** (it's still useful)
2. **Simplify the webhook API** (remove real-time processing)
3. **Add recent context functions** (simple SQL)
4. **Modify N8N workflows** (change to 2-hour cron)
5. **Update chat API** (inject recent context only)

### **Gradual Enhancement:**
1. **Week 1:** Recent context working
2. **Week 2:** N8N background processing
3. **Week 3:** Historical search
4. **Week 4:** Performance optimization

## **üí° WHY THIS IS OPTIMAL:**

### **Performance:**
- **Chat feels instant** (no waiting for AI processing)
- **Scales linearly** with user count
- **Predictable latency** (no AI API variability)

### **Reliability:**
- **Chat works** even if N8N is down
- **Graceful degradation** (recent context always available)
- **Simple debugging** (fewer moving parts)

### **Cost:**
- **Lower OpenAI costs** (batch processing)
- **Lower infrastructure costs** (less compute during peak)
- **Better resource utilization** (background processing during off-hours)

**This architecture gives you the best of both worlds: instant chat performance with rich contextual intelligence.**
